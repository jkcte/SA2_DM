{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45830c2f",
   "metadata": {},
   "source": [
    "# Churn Prediction Model Analysis\n",
    "This notebook analyzes and compares multiple models for predicting customer churn in a telecom dataset. It includes preprocessing, model training, evaluation, and feature importance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20805612",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd98375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('churn-bigml-80.csv')\n",
    "\n",
    "# Drop non-predictive column and encode binary values\n",
    "df = df.drop(['Account length'], axis=1)\n",
    "df = df.replace({'Yes': 1, 'No': 0})\n",
    "df_encoded = pd.get_dummies(df, columns=['State', 'Area code'])\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bdc46a",
   "metadata": {},
   "source": [
    "We load the dataset, drop the non-informative 'Account length' feature, and encode categorical features using one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e6cde",
   "metadata": {},
   "source": [
    "## Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90914491",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop('Churn', axis=1)\n",
    "y = df_encoded['Churn'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd55396",
   "metadata": {},
   "source": [
    "Here, we separate the feature matrix `X` and the target variable `y` for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09767c3f",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d523841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Train models\n",
    "logreg = LogisticRegression(max_iter=1000).fit(X, y)\n",
    "ridge_best = Ridge(alpha=1.0, max_iter=10000).fit(X, y)\n",
    "lasso_best = Lasso(alpha=0.1, max_iter=10000).fit(X, y)\n",
    "dtree = DecisionTreeClassifier(random_state=42).fit(X, y)\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100).fit(X, y)\n",
    "gbc = GradientBoostingClassifier(random_state=42, n_estimators=100).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d04cf06",
   "metadata": {},
   "source": [
    "We train six models: Logistic Regression, Ridge, Lasso, Decision Tree, Random Forest, and Gradient Boosting using the preprocessed training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ac4e0",
   "metadata": {},
   "source": [
    "## Load and Prepare Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDF = pd.read_csv('churn-bigml-20.csv')\n",
    "testingDF = testingDF.drop(['Account length'], axis=1)\n",
    "testingDF = testingDF.replace({'Yes': 1, 'No': 0})\n",
    "testingDF = pd.get_dummies(testingDF, columns=['State', 'Area code'])\n",
    "\n",
    "# Align columns with training set\n",
    "missing_cols = set(X.columns) - set(testingDF.columns)\n",
    "for col in missing_cols:\n",
    "    testingDF[col] = 0\n",
    "testingDF = testingDF[X.columns]\n",
    "X_test = testingDF.drop('Churn', axis=1)\n",
    "y_test = testingDF['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b819a",
   "metadata": {},
   "source": [
    "We load and preprocess the test set in the same way as the training data, ensuring feature alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7cc7c",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580456bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Logistic Regression\\n\", classification_report(y_test, logreg.predict(X_test)))\n",
    "print(\"Ridge Regression\\n\", classification_report(y_test, (ridge_best.predict(X_test) >= 0.5).astype(int)))\n",
    "print(\"Lasso Regression\\n\", classification_report(y_test, (lasso_best.predict(X_test) >= 0.5).astype(int)))\n",
    "print(\"Decision Tree\\n\", classification_report(y_test, dtree.predict(X_test)))\n",
    "print(\"Random Forest\\n\", classification_report(y_test, rf.predict(X_test)))\n",
    "print(\"Gradient Boosting\\n\", classification_report(y_test, gbc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd70d67",
   "metadata": {},
   "source": [
    "We evaluate each model using the `classification_report`, which includes precision, recall, f1-score, and support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67647c86",
   "metadata": {},
   "source": [
    "## Feature Importance and Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac808c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_data = {\n",
    "    'Logistic Regression': pd.Series(logreg.coef_[0], index=X.columns),\n",
    "    'Ridge Regression': pd.Series(ridge_best.coef_, index=X.columns),\n",
    "    'Lasso Regression': pd.Series(lasso_best.coef_, index=X.columns),\n",
    "    'Decision Tree': pd.Series(dtree.feature_importances_, index=X.columns),\n",
    "    'Random Forest': pd.Series(rf.feature_importances_, index=X.columns),\n",
    "    'Gradient Boosting': pd.Series(gbc.feature_importances_, index=X.columns),\n",
    "}\n",
    "\n",
    "importance_df = pd.DataFrame(importance_data)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595ee6c",
   "metadata": {},
   "source": [
    "This table shows the feature coefficients for regression models and feature importances for tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25060b",
   "metadata": {},
   "source": [
    "## Summary and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f30d6",
   "metadata": {},
   "source": [
    "### Best Performing Model\n",
    "Based on F1-score and balanced performance across precision and recall, **Gradient Boosting** often yields the best performance. It balances overfitting and generalization by building trees sequentially.\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "- **Logistic, Ridge, Lasso**: High bias, low variance â€” simpler models but may underfit complex patterns.\n",
    "- **Tree-based models**: Decision Trees have high variance and overfit easily, while Random Forest and Gradient Boosting reduce variance using ensemble strategies.\n",
    "\n",
    "### Interpretability vs. Predictive Power\n",
    "- **Regression models**: More interpretable, with coefficients directly linked to features.\n",
    "- **Tree ensembles**: Less interpretable but usually more powerful in prediction due to capturing nonlinear relationships.\n",
    "\n",
    "### Real-world Implications\n",
    "A telecom company can use these models to:\n",
    "- **Identify customers likely to churn** and proactively offer incentives.\n",
    "- **Understand key drivers** of churn using feature importances (e.g., 'International plan', 'Total day minutes').\n",
    "- **Segment customers** for targeted retention strategies, reducing revenue loss and improving satisfaction.\n",
    "\n",
    "Ultimately, the model selection depends on whether the priority is **accuracy** (Gradient Boosting) or **interpretability** (Logistic Regression or Lasso)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
